---
releaseDate: "2021-02-15"
title: "Setting up a startup's staging environment"
category: "engineering"
tags: ["devops", "design"]
description: "How to settup a simple staging environment based on a gitops approach coupled with a k8s infrastructure"
---

_To illustrate this article, you can find the github repository
[Dot-H/simple-staging-env-ex](https://github.com/Dot-H/simple-staging-env-ex)_

At eKee, when the team and the number of users started growing, we had an issue.
We wanted to test things out and propose different versions of the product to
the users. Issue being we were a startup and duplicating the environment was way
to expensive.

In this article, I'll shortly explain our backend architecture, the solution I
went to, the tools used to develop it and its limitations.

## The backend environment

At eKee we had an **hybrid architecture**. Without the fancy words, it just means
that our stateful services (_databases_, _knowledge base software_, _gitlab_...)
were running on on-premise servers while the stateless services
(_engine_, _api_, _websites_...) were running in a cloud infrastructure using
kubernetes.

Everything was deployed using a gitops approach, taging a new version of a service
would trigger a new deployment. In order to do so, we were using
[**ansible**](https://docs.ansible.com/ansible/latest/index.html) for on-premise
services and [**helm**](https://helm.sh/) coupled with
[**argocd**](https://argo-cd.readthedocs.io/en/stable/) for the in-cloud services.

## The specification

Based on this environment, here is what I wanted to develop:

- Expose a `staging.[service]` subdomain for each service which would resolve to
  a specific version of this service;
- The version of a service could be any of its branch on git. If I want to test
  _feature-A_ on the _ekee-api_ service I would deploy the latest commit on branch
  _feature-A_;
- Each service in the staging environment can use a different branch. They can be
  deployed independantly;
- The staging environment relies as much as it can on the prod one, no extra
  deployment needed;
- Deployments must be **simple**. If the developers needs to do more than two
  operations, they won't use it (we all know we are lazy kind of people).

Following this specification, here is the developer experience I wanted:

1. **Developer A** develops a new interface to share documents. The branch
   _doc-sharing-v2_ is created in the repository _ekee-dashboard_,
1. After commiting some beautiful engineering, **Developer A** opens a pull
   request and labels it _staging_,
1. Once the CI returns no error, **Developer A** clicks _deploy_ in _argocd_,
1. When the PR is closed and/or the label **staging** is removed, the resources
   used by in the staging environment are released.

## The implementation

### Adding the workflow around the _staging_ label in github

To trigger special workflows upon specific events, github exposes a wonderful
set of tools through what they call [github actions](https://docs.github.com/en/actions).

Here is the action we need to tag the latest PR's commit when the _staging_ label
is present and untag it when the PR is closed or unlabeled:

```yaml
name: Release staging version
on:
  pull_request:
    types: [opened, synchronize, reopened, labeled]
jobs:
  tag:
    if: ${{ !contains(github.event.pull_request.labels.*.name, 'no-ci') }}
    name: Release staging
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 1
      # Retrieve the branch name when there is a `staging` label
      - name: Extract branch name
        shell: bash
        run: echo "branch=${GITHUB_HEAD_REF}" >> $GITHUB_OUTPUT
        id: extract_branch
      # Tag the current commit with `staging-BRANCH_NAME` when there is a `staging` label
      - name: "Tag staging"
        uses: eKee-io/git-tag-action@fix-not-a-git-dir
        if:
          ${{ !contains(fromJson('["unlabeled", "closed"]'), github.event.action)
          && contains(github.event.pull_request.labels.*.name, 'staging') }}
        env:
          TAG: staging-${{ steps.extract_branch.outputs.branch }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          BRANCH: ${{ steps.extract_branch.outputs.branch }}
```

### Building the staging image to deploy

Nothing fancy here! What we deploy is a docker image so the strategy is pretty simple:

1. Put a Dockerfile in the root directory of the services,
1. Add an action triggered on tagging events building the docker image,
1. Tag the docker image with the same tag as git,
1. Push this image to your registry in order for it to be accessible by
   your cloud resources.

To illustrate this, I used the github container registry in
[Dot-H/simple-staging-env-ex](https://github.com/Dot-H/simple-staging-env-ex).

With the github workflows I had an issue I didn't have with gitlab: a workflow inside a
branch does not seem to be able to trigger an other one without the explicit
[workflow_run](https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#workflow_run)
action..

So no fancy linking through the tag event possible. We need to use this action in the
[build_image.yaml](https://github.com/Dot-H/simple-staging-env-ex):

```yaml
name: Build docker image
on:
  workflow_run:
    workflows: ["Release staging version"]
    types:
      - completed
env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
jobs:
  deploy:
    name: Build the docker image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        # The workflow runs on the main branch, we need to checkout to the
        # commit which triggered the `workflow_run` action
        with:
          ref: ${{ github.event.workflow_run.head_sha }}
      - name: Log in to the Container registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }} # Login to ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      # Nicely format the tags & labels. This maybe useful if, as for me,
      # your actor name contains uppercase letters
      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=raw,value=staging-${{ github.event.workflow_run.head_branch }}
      - name: Build and push Docker image
        uses: docker/build-push-action@v3
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
```

### Deploying the resources in k8s

Once the staging image is built and present on your container registry, it
should be available to your infrastructure.

In order for the infrastructure to handle the deployment of the staging
version, what we need is add a staging version for the [_deployment_](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
, the [_service_](https://kubernetes.io/docs/concepts/services-networking/service/)
and potentially exposing through an [_ingress_](https://kubernetes.io/docs/concepts/services-networking/service/).

To do so without having to rewrite everything, I use [helm](https://helm.sh/)
and its templates. I won't explain the helm chart I developed for the example
in details here but rather the small helm tricks I used. Take a glance [here](https://github.com/Dot-H/simple-staging-env-ex/tree/main/k8s/staging-env)
to illustrate what I'm saying.

#### Staging specific values

The helm staging values are defined in the [`values-staging.yaml`](https://github.com/Dot-H/simple-staging-env-ex/blob/main/k8s/staging-env/values-staging.yaml)
and by default extend [`values.yaml`](https://github.com/Dot-H/simple-staging-env-ex/blob/main/k8s/staging-env/values.yaml).

The staging value file is composed as follow:

```yaml
nameOverride: "NAME"
image:
  pullPolicy: Always
staging:
  tag: staging-BRANCH_NAME
  enabled:
    echo-server: true
```

The `staging.enabled` controls which services compose the staging deployment
by using the `staging.tag`. When used, the tag is expected to be available
on the cloud repository.

The `nameOverride` permits to give a name to the created resources in the
staging environment. This avoids them to conflict with the production ones.
You may wonder why we don't rely on namespaces here. The main reason is that
it makes sharing resources between the staging environment and the production
one harder and the configuration less clear. With this approach, having staging
versions using only a subset of the real environment is way simpler.

The `image.pullPolicy: Always` causes the staging pods to always pull their
images. This is important because our building process does not change the tag
when a new version is deployed, it just publishes a new image. Therefore we
want the pods to always pull to make sure they always have the latest version.

#### Make staging elements use production resources

There is one downside to the `ekee.helm` approach. Since **all** the resources names starts with the [`ekee.name`](https://github.com/Dot-H/ekee-k8s/blob/master/ekee/templates/_helpers.tpl) prefix, the secrets and configs expected by the staging elements should also
starts with it. Yet, as we said previously, we don't deploy anything other than the `Service` and `Deployment` resources. Therefore, the
templates must contain a way to say to `ekee.helm` "Do not use the `.Values.nameOverride` here". To do so, there is a new template [`ekee.strictName`](https://github.com/Dot-H/ekee-k8s/blob/master/ekee/templates/_helpers.tpl) which is used like

```yaml
- name: EKEE_AUTH_DB_USER_NAME
  valueFrom:
    secretKeyRef:
      name: {{ template "ekee.strictName" . }}-db-oauth2-secrets
      key: username
```

#### Ignored resources

The ignored resources are wrapped in a `if` clause at the beginning of the files to ignore ([exemple](https://github.com/Dot-H/ekee-k8s/blob/master/ekee/templates/10-namespace.yaml#L1)).

there is a `.Values.staging.tag` defined and does nothing if so.

#### Deploy a staging environment
